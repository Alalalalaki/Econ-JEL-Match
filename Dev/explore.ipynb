{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70c2283-ddb9-4bf0-92c1-1f16ca1c6ca5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3c94d8fa-9ded-4e45-8415-a51338165b46",
   "metadata": {},
   "source": [
    "jel: https://www.aeaweb.org/econlit/jelCodes.php?view=jel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc45242-970d-4c69-9671-fb5f91f280e3",
   "metadata": {},
   "source": [
    "ref (general):\n",
    "- https://scikit-learn.org/stable/modules/multiclass.html#multilabel-classification\n",
    "- https://www.analyticsvidhya.com/blog/2017/08/introduction-to-multi-label-classification/\n",
    "- https://towardsdatascience.com/journey-to-the-center-of-multi-label-classification-384c40229bff\n",
    "- https://keras.io/examples/nlp/multi_label_classification/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f709db-de80-456b-868f-2ffa632ca705",
   "metadata": {},
   "source": [
    "ref (few-shot):\n",
    "- https://maelfabien.github.io/machinelearning/NLP_5/\n",
    "- https://few-shot-text-classification.fastforwardlabs.com/\n",
    "- https://research.aimultiple.com/few-shot-learning/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b7848a-7cfd-4f33-9025-ed1d87f07a46",
   "metadata": {},
   "source": [
    "**Note**:\n",
    "- use paper database to do multi-label classification does not work as I thought\n",
    "  - the problem is that too many lables and for some lables there are only very limited number of papers, which creating large imbalanced data problem\n",
    "  - this problem is not solved by using sample adjusting like SMOTE, and only partly solved by using class_weight = \"balanced\": not very good prediction: in test sample, some true label is missing, I feel that > 50% correctness finding in top20-30\n",
    "  \n",
    "**TO-DO:**\n",
    "  - Add more data \n",
    "  - Try adding penality in L2 logistic\n",
    "  - Try SVM\n",
    "  - Use the information of jel to do alternative match \n",
    "  - Combine two methods to increase prediction power"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1dbfed-7b5d-409f-b4d9-5b48dfb6648f",
   "metadata": {},
   "source": [
    "## Data Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef7f5b7a-fd1c-417e-9df3-3b91ec446dde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(159068, 9)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"/Users/alalalalaki/GitHub/Econ-Paper-Search/Data/papers.csv\")\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "670770ed-c1dc-40ff-bd31-db522be13d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "## borrow from Econ-Paper-Search\n",
    "# drop book reviews (not perfect)\n",
    "masks = [~df.title.str.contains(i, case=False, regex=False) for i in [\"pp.\", \" p.\"]]  # \"pages,\" \" pp \"\n",
    "mask = np.vstack(masks).all(axis=0)\n",
    "df = df.loc[mask]\n",
    "# drop some duplicates due to weird strings in authors and abstract\n",
    "df = df[~df.duplicated(['title', 'url']) | df.url.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8eaacfc4-e595-4d79-9154-c324aea21628",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = ~df.jel.isna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f1ee1e0-a124-4aa1-8c92-ca78bd8e6052",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask2 = df.year > 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10f77d11-6792-4d1a-8da1-0ba57dab1aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask3 = ~df.abstract.isna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5305b48-ebc9-4d63-b80e-f8b68090033f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44880, 9)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[mask & mask2 & mask3]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90d21a7d-9a67-4ee6-8f8a-1fca0e876063",
   "metadata": {},
   "outputs": [],
   "source": [
    "jel_dummy_matrix = df.jel.str.get_dummies(sep=\"&\")\n",
    "\n",
    "# use only 1digit\n",
    "# jel_dummy_matrix = (df.jel.str.replace(\"(?<=[A-Z]\\d)\\d\",\"\", regex=True)\n",
    "#                           .str.get_dummies(sep=\"&\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d3e9e8-1bc3-4e6f-ae0f-f6159c8b322a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many times a jel code is used in the dataset\n",
    "# jel_dummy_matrix.sum(axis=0).plot.hist(bins=1000)\n",
    "jel_dummy_matrix.sum(axis=0).value_counts().sort_index()\n",
    "\n",
    "# how many jel codes in each paper\n",
    "#df.jel.str.count(\"&\").plot.hist(bins=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc0c5809-9e5d-48fd-a495-dff070b6626b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove not 2-digit jel\n",
    "jel_dummy_matrix = jel_dummy_matrix.loc[:,~(jel_dummy_matrix.columns.str.len()<3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce627ef4-dedc-4549-bc57-cce298b2da70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove label with too less entry\n",
    "label_idx_minor = jel_dummy_matrix.sum(axis=0) <= 1 \n",
    "label_idx_minor.sum()\n",
    "jel_dummy_matrix = jel_dummy_matrix.loc[:,~label_idx_minor]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "936dadd3-8c24-4ec8-967b-fa4f9b5017a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "jel_labels = jel_dummy_matrix.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a96f4767-6b42-4510-9382-507816a49a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# further remove papers with no jel now\n",
    "mask = jel_dummy_matrix.sum(axis=1) == 0\n",
    "jel_dummy_matrix = jel_dummy_matrix.loc[~mask,:]\n",
    "df = df.loc[~mask,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "747e9476-8fd4-4029-a6f0-d8f95acb62f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "\n",
    "#stop_words = set(stopwords.words('english'))\n",
    "#stop_words.update(['zero','one','two','three','four','five','six','seven',\n",
    "# 'eight','nine','ten','may','also','across','among','beside','however','yet','within'])\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "def stemming(sentence):\n",
    "    stemSentence = \"\"\n",
    "    for word in sentence.split():\n",
    "        stem = stemmer.stem(word)\n",
    "        stemSentence += stem\n",
    "        stemSentence += \" \"\n",
    "    stemSentence = stemSentence.strip()\n",
    "    return stemSentence\n",
    "\n",
    "abstract = df.abstract.apply(stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dd84c4f9-c715-4cf8-9188-7100710e966c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f0769b29-ba3a-4340-a9c8-2b9854012652",
   "metadata": {},
   "source": [
    "vect = CountVectorizer(stop_words='english',min_df=2,)\n",
    "token_matrix = vect.fit_transform(df.abstract)\n",
    "token_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f7d8d637-70cf-497f-b804-0ba578286a17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<42052x26278 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 2382098 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(stop_words='english',min_df=2,\n",
    "                        # strip_accents='unicode',\n",
    "                        # ngram_range=(1,2), norm='l2',\n",
    "                       )\n",
    "token_matrix = tfidf.fit_transform(abstract)\n",
    "token_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "daf7b90e-09db-438c-938c-8d970f7f8bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tfidf.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "f045df90-b075-444c-847d-47e885c72d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y_train, y_test, X_train, X_test = train_test_split(jel_dummy_matrix, token_matrix, \n",
    "                               random_state=46,\n",
    "                               test_size=0.005, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb0bc09-4c32-4620-bbf2-7fb56fa52c9e",
   "metadata": {},
   "source": [
    "## Try Supervised Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e7af93-83dd-4c09-a362-6ccf853f25c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83147e8-4440-47db-9588-4a138d8c8633",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_test_random(classifier, proba=False):\n",
    "    rand_n = np.random.choice(X_test.shape[0])\n",
    "\n",
    "    X_test_rand = X_test[rand_n,:]\n",
    "\n",
    "    y_test_rand = y_test.iloc[rand_n,:] \n",
    "    print(\"Real JELs: \", y_test_rand[y_test_rand > 0])\n",
    "    \n",
    "    if not proba:\n",
    "        predict_rand = classifier.predict(X_test_rand) \n",
    "        print(\"Predict JELs: \", predict_rand[predict_rand > 0])\n",
    "    else:\n",
    "        predict_proba_rand = classifier.predict_proba(X_test_rand) \n",
    "        print(\"Predict JELs: \", predict_rand[predict_rand > 0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "090b0b85-4565-431d-adcc-299057518a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_metric(classifer, X_test, y_test, top_n=10):\n",
    "    find_rates = []\n",
    "    for i in tqdm(range(len(y_test))):\n",
    "        X_test_ = X_test[i,:]\n",
    "        y_test_ = y_test.iloc[i,:]\n",
    "        y_test_jel = y_test_[y_test_ > 0].index.values\n",
    "\n",
    "        predict_proba_ = classifier.predict_proba(X_test_) \n",
    "        predict_proba_ = np.array([a[0][1] for a in predict_proba_])\n",
    "        jel_rank = np.argsort(predict_proba_)[::-1]\n",
    "        if top_n <=1:\n",
    "            top_n = (predict_proba_ > top_n).sum()\n",
    "        y_predict_jel = jel_labels[jel_rank][:top_n]\n",
    "        y_predict_jel_proba = predict_proba_[jel_rank][:top_n]\n",
    "\n",
    "        base = len(y_test_jel)\n",
    "        correct_find = len(set(y_predict_jel) & set(y_test_jel))\n",
    "        find_rate = correct_find/base\n",
    "        find_rates.append(find_rate)\n",
    "    avg_find_rate = np.array(find_rates).mean()\n",
    "    return avg_find_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eaca79b-6e94-49e6-b7b2-97e74495ff0b",
   "metadata": {},
   "source": [
    "### ~~OneVsRest~~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1817bdc-f187-4ee3-b0ae-cc7851b2c6a1",
   "metadata": {},
   "source": [
    "-  An intuitive approach to solving multi-label problem is to decompose it into multiple independent binary classification problems (one per category).\n",
    "- In an “one-to-rest” strategy, one could build multiple independent classifiers and, for an unseen instance, choose the class for which the confidence is maximized.\n",
    "- The main assumption here is that the labels are mutually exclusive. You do not consider any underlying correlation between the classes in this method. For instance, it is more like asking simple questions, say, “is the comment toxic or not”, “is the comment threatening or not?”, etc. Also there might be an extensive case of overfitting here, since most of the comments are unlabeled, i,e., most of the comments are clean comments.\n",
    "- @not work, simply predict all 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45bad345-9aa3-49a2-9c8c-814d03a62192",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf197d4-911d-4c49-91aa-559308c178bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifer = OneVsRestClassifier(LogisticRegression(solver='sag'), n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645d7796-6cf5-4333-9785-b82516035b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifers = []\n",
    "# predictions = []\n",
    "for jel in tqdm(jel_dummy_matrix.columns):    \n",
    "    # Training logistic regression model on train data\n",
    "    y_train_ = y_train[jel]\n",
    "    classifer.fit(X_train, y_train_,)\n",
    "    classifers.append(LogReg_pipeline)\n",
    "    # ~~calculating test accuracy~~ this makes no sense for imbalanced data\n",
    "    # prediction = LogReg_pipeline.predict(X_test)\n",
    "    # predictions.append(prediction)\n",
    "    # print('Test accuracy is {}'.format(accuracy_score(y_test[jel], prediction)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d425bd30-fc77-4937-a128-2ae478e23c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_n = np.random.choice(X_test.shape[0])\n",
    "\n",
    "X_test_rand = X_test[rand_n,:]\n",
    "\n",
    "y_test_rand = y_test.iloc[rand_n,:] \n",
    "y_test_rand[y_test_rand > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280d0106-b6de-4268-bfab-b35d5fd2eb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = [c.predict(X_test_rand) for c in classifers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f351cbb-fdc7-4648-aa83-5cb6ed77467e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e570545-50e3-4117-9f83-f9644a7b207c",
   "metadata": {},
   "source": [
    "### ~~Binary Relevance~~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a47d36-4b19-442c-81f5-d8fc462ab8f9",
   "metadata": {},
   "source": [
    "- In this case an ensemble of single-label binary classifiers is trained, one for each class. Each classifier predicts either the membership or the non-membership of one class. The union of all classes that were predicted is taken as the multi-label output. This approach is popular because it is easy to implement, however it also ignores the possible correlations between class labels.\n",
    "- In other words, if there’s q labels, the binary relevance method create q new data sets from the images, one for each label and train single-label classifiers on each new data set. One classifier may answer yes/no to the question “does it contain trees?”, thus the “binary” in “binary relevance”. This is a simple approach but does not work well when there’s dependencies between the labels.\n",
    "- OneVsRest & Binary Relevance seem very much alike. If multiple classifiers in OneVsRest answer “yes” then you are back to the binary relevance scenario.\n",
    "- @Never ends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c5a834-8ef7-4d15-bfad-a875e4ee722c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8105fc90-df68-4276-8e25-d37615528fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = BinaryRelevance(GaussianNB())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce15fde-1280-4324-8f9f-bf53acb2ab0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5574443d-69c9-4cdf-b565-6442d1da3b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = classifier.predict(X_test)\n",
    "print(\"Accuracy = \",accuracy_score(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e59b580-e2b9-4be8-ba19-af8632fbd18a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7a46c734-a6e6-4833-ba17-1ab362497c88",
   "metadata": {},
   "source": [
    "### RandomForest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb14f6e-3a03-4248-924d-b24388a7551c",
   "metadata": {},
   "source": [
    "#### MultiOutputClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86d38f4-1485-4ccb-8f42-28b8a7aa335e",
   "metadata": {},
   "source": [
    "- Multilabel classification support can be added to any classifier with MultiOutputClassifier.  This strategy consists of fitting one classifier per target. This allows multiple target variable classifications. \n",
    "- The purpose of this class is to extend estimators to be able to estimate a series of target functions (f1,f2,f3…,fn) that are trained on a single X predictor matrix to predict a series of responses (y1,y2,y3…,yn).\n",
    "- @again can only predict 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62669b07-9541-4422-ba9d-64115f8a33e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61937a09-6d6c-4203-b5bb-12ce6e9fea92",
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(random_state=46)\n",
    "classifier = MultiOutputClassifier(forest, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bf8a38-e5a3-4056-ac0e-c892553e6696",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2019e1d-c2f8-434f-91d9-07aeb2ab6adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = classifier.predict(X_test)\n",
    "print(\"Accuracy = \",accuracy_score(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a337e0-862c-48e1-95e9-d95c55b1a431",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f250a415-489b-41e5-87dc-d1f316a8bfb2",
   "metadata": {},
   "source": [
    "#### BinaryRelevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c46e41-efcf-4f5c-8925-c616bdf11023",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc3e111-ba38-4734-85cc-bb62f06d60e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = BinaryRelevance(\n",
    "    classifier = RandomForestClassifier(),\n",
    "    require_dense = [False, True],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0000563d-ddc6-4f74-9356-24294c1798b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20b4b6c-4643-4082-8c6c-958d361d0aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_test_random(classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802af2fd-8f1b-4555-a4e8-0b0ec3df22ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dbd11eda-002f-47a8-942b-6cf3d6ffb3cb",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Classifier Chains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f83ebc3-b0be-423f-9faa-52f625a96aa9",
   "metadata": {},
   "source": [
    "- A chain of binary classifiers C0, C1, . . . , Cn is constructed, where a classifier Ci uses the predictions of all the classifier Cj , where j < i. This way the method, also called classifier chains (CC), can take into account label correlations.\n",
    "- The total number of classifiers needed for this approach is equal to the number of classes, but the training of the classifiers is more involved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8d63e3-8e54-4a98-8a06-7380e4e1e864",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skmultilearn.problem_transform import ClassifierChain\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2034d071-74eb-46b2-921e-c116f09d1b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = ClassifierChain(LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133ce240-f118-4c10-9713-721d6fad8861",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2f29ae-55ac-46dc-b6f6-fd1b35176cdd",
   "metadata": {},
   "source": [
    "### ~~Label Powerset~~\n",
    "\n",
    "- This approach does take possible correlations between class labels into account. It considers each member of the power set of labels in the training set as a single label.\n",
    "- This method needs worst case (2^|C|) classifiers, and has a high computational complexity.\n",
    "- However *when the number of classes increases the number of distinct label combinations can grow exponentially. This easily leads to combinatorial explosion and thus computational infeasibility.* Furthermore, some label combinations will have very few positive examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f522b5-53cf-412e-9f02-4dbb909b66ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skmultilearn.problem_transform import LabelPowerset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3fba31-9230-4882-a332-1c202c2d17ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = LabelPowerset(LogisticRegression())\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ba4dbf-1566-4e48-a1db-dcfde102ac80",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = classifier.predict(X_test)\n",
    "print(\"Accuracy = \",accuracy_score(y_test,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00951a19-d42c-4d9c-809f-b8ee1675d408",
   "metadata": {},
   "source": [
    "### ~~Adapted Algorithm~~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aef2980-ec1c-445a-9f0b-7e5d8ec57d16",
   "metadata": {},
   "source": [
    "- Algorithm adaptation methods for multi-label classification concentrate on adapting single-label classification algorithms to the multi-label case usually by changes in cost/decision functions.\n",
    "- Here we use a multi-label lazy learning approach named ML-KNN which is derived from the traditional K-nearest neighbor (KNN) algorithm.\n",
    "- The skmultilearn.adapt module implements algorithm adaptation approaches to multi-label classification, including but not limited to ML-KNN.\n",
    "- *Both ML-KNN and label-powerset take considerable amount of time* when run on this dataset, so experimentation was done on a random sample of the train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8548fd5-8a90-413d-8289-aece1180f934",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skmultilearn.adapt import MLkNN\n",
    "\n",
    "# Note that this classifier can throw up errors when handling sparse matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bb59d0-df8b-4fa0-a32c-ec36db403ad9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c71f8f45-c409-4ddf-b9e3-9764b1990f1b",
   "metadata": {},
   "source": [
    "### Solve Imbalanced Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208764f4-c404-4705-92b2-a92dc9937f54",
   "metadata": {},
   "source": [
    "#### MLSMOTE\n",
    "\n",
    "https://medium.com/thecyphy/handling-data-imbalance-in-multi-label-classification-mlsmote-531155416b87\n",
    "\n",
    "- @code only works for pandas df, and it seems does not fit well to very large amount of labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f61b716-623d-475d-9646-18d6d1507439",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb456f3-2f5e-4fac-8b50-39d3369df02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tail_label(df):\n",
    "    \"\"\"\n",
    "    Give tail label colums of the given target dataframe\n",
    "    \n",
    "    args\n",
    "    df: pandas.DataFrame, target label df whose tail label has to identified\n",
    "    \n",
    "    return\n",
    "    tail_label: list, a list containing column name of all the tail label\n",
    "    \"\"\"\n",
    "    columns = df.columns\n",
    "    n = len(columns)\n",
    "    irpl = np.zeros(n)\n",
    "    for column in range(n):\n",
    "        irpl[column] = df[columns[column]].value_counts()[1]\n",
    "    irpl = max(irpl)/irpl\n",
    "    mir = np.average(irpl)\n",
    "    tail_label = []\n",
    "    for i in range(n):\n",
    "        if irpl[i] > mir:\n",
    "            tail_label.append(columns[i])\n",
    "    return tail_label\n",
    "\n",
    "def get_index(df):\n",
    "  \"\"\"\n",
    "  give the index of all tail_label rows\n",
    "  args\n",
    "  df: pandas.DataFrame, target label df from which index for tail label has to identified\n",
    "    \n",
    "  return\n",
    "  index: list, a list containing index number of all the tail label\n",
    "  \"\"\"\n",
    "  tail_labels = get_tail_label(df)\n",
    "  index = set()\n",
    "  for tail_label in tail_labels:\n",
    "    sub_index = set(df[df[tail_label]==1].index)\n",
    "    index = index.union(sub_index)\n",
    "  return list(index)\n",
    "\n",
    "def get_minority_instace(X, y):\n",
    "    \"\"\"\n",
    "    Give minority dataframe containing all the tail labels\n",
    "    \n",
    "    args\n",
    "    X: pandas.DataFrame, the feature vector dataframe\n",
    "    y: pandas.DataFrame, the target vector dataframe\n",
    "    \n",
    "    return\n",
    "    X_sub: pandas.DataFrame, the feature vector minority dataframe\n",
    "    y_sub: pandas.DataFrame, the target vector minority dataframe\n",
    "    \"\"\"\n",
    "    index = get_index(y)\n",
    "    X_sub = X[X.index.isin(index)].reset_index(drop = True)\n",
    "    y_sub = y[y.index.isin(index)].reset_index(drop = True)\n",
    "    return X_sub, y_sub\n",
    "\n",
    "def nearest_neighbour(X):\n",
    "    \"\"\"\n",
    "    Give index of 5 nearest neighbor of all the instance\n",
    "    \n",
    "    args\n",
    "    X: np.array, array whose nearest neighbor has to find\n",
    "    \n",
    "    return\n",
    "    indices: list of list, index of 5 NN of each element in X\n",
    "    \"\"\"\n",
    "    nbs=NearestNeighbors(n_neighbors=5,metric='euclidean',algorithm='kd_tree').fit(X)\n",
    "    euclidean,indices= nbs.kneighbors(X)\n",
    "    return indices\n",
    "\n",
    "def MLSMOTE(X,y, n_sample):\n",
    "    \"\"\"\n",
    "    Give the augmented data using MLSMOTE algorithm\n",
    "    \n",
    "    args\n",
    "    X: pandas.DataFrame, input vector DataFrame\n",
    "    y: pandas.DataFrame, feature vector dataframe\n",
    "    n_sample: int, number of newly generated sample\n",
    "    \n",
    "    return\n",
    "    new_X: pandas.DataFrame, augmented feature vector data\n",
    "    target: pandas.DataFrame, augmented target vector data\n",
    "    \"\"\"\n",
    "    indices2 = nearest_neighbour(X)\n",
    "    n = len(indices2)\n",
    "    new_X = np.zeros((n_sample, X.shape[1]))\n",
    "    target = np.zeros((n_sample, y.shape[1]))\n",
    "    for i in range(n_sample):\n",
    "        reference = random.randint(0,n-1)\n",
    "        neighbour = random.choice(indices2[reference,1:])\n",
    "        all_point = indices2[reference]\n",
    "        nn_df = y[y.index.isin(all_point)]\n",
    "        ser = nn_df.sum(axis = 0, skipna = True)\n",
    "        target[i] = np.array([1 if val>2 else 0 for val in ser])\n",
    "        ratio = random.random()\n",
    "        gap = X.loc[reference,:] - X.loc[neighbour,:]\n",
    "        new_X[i] = np.array(X.loc[reference,:] + ratio * gap)\n",
    "    new_X = pd.DataFrame(new_X, columns=X.columns)\n",
    "    target = pd.DataFrame(target, columns=y.columns)\n",
    "    new_X = pd.concat([X, new_X], axis=0)\n",
    "    target = pd.concat([y, target], axis=0)\n",
    "    return new_X, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2182c971-02c2-4fab-b608-92f2f8a7a647",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = get_index(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed7d43e-7d85-43ff-a12b-1aa2881dc81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sub, y_sub = get_minority_instace(X_train, y_train)   #Getting minority instance of that datframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e7ce12-74dd-4d58-9214-0ac120421996",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_res,y_res =MLSMOTE(X_sub, y_sub, 100)     #Applying MLSMOTE to augment the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e234489-5dfc-440b-a038-8e5298efb2d8",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0bfebf-fd4d-4525-b871-734bf652178c",
   "metadata": {},
   "source": [
    "- @somehow doesnot help at all, I guess this's about overfitting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffda559-f7e8-4544-95d6-b25cd21628a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc47c9ac-5768-4103-8ac6-a51df0d60a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c95dca-6bb4-42a7-8094-39d8da4c9e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "oversample = SMOTE(k_neighbors=2, random_state=46)\n",
    "classifer = LogisticRegression(solver='sag', max_iter=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4515b0da-7bdf-4d02-813f-8156ea433ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifers = []\n",
    "# predictions = []\n",
    "for jel in tqdm(jel_dummy_matrix.columns):    \n",
    "    # Training logistic regression model on train data\n",
    "    y_train_ = y_train[jel]\n",
    "    X_train_, y_train_ = oversample.fit_resample(X_train, y_train_)\n",
    "    \n",
    "    t = classifer.fit(X_train_, y_train_,)\n",
    "    classifers.append(classifer)\n",
    "    # ~~calculating test accuracy~~ this makes no sense for imbalanced data\n",
    "    # prediction = LogReg_pipeline.predict(X_test)\n",
    "    # predictions.append(prediction)\n",
    "    # print('Test accuracy is {}'.format(accuracy_score(y_test[jel], prediction)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3cb4c9-b4aa-4dd4-bb9a-f6b095b10dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_n = np.random.choice(X_test.shape[0])\n",
    "X_test_rand = X_test[rand_n,:]\n",
    "y_test_rand = y_test.iloc[rand_n,:] \n",
    "print(\"Real JELs: \", y_test_rand[y_test_rand > 0])\n",
    "\n",
    "predict_rand = np.array([c.predict(X_test_rand) for c in classifers]).flatten() \n",
    "predict_rand = np.array([c.predict_proba(X_test_rand)[:,1] for c in classifers]).flatten() \n",
    "print(\"Predict JELs: \", predict_rand[predict_rand > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e938178c-88ab-4672-beee-af17cd2ace86",
   "metadata": {},
   "source": [
    "#### class_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35247be-7c2a-4a8f-adf5-ec0794cad0f8",
   "metadata": {},
   "source": [
    "- @Still not work: predict many 1 and not accurate at all\n",
    "- @This problem even remains when only checking 1digit (not full 2digit), now it's quite accurate that you can find the true jel code in to top 20 predictions, but ...\n",
    "- @Using penality=\"l1\" many not-converge and thus not high prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ca8e4acd-37be-47e8-8171-9c7ace1f0fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "12c0510d-e373-4782-bee8-7c9ccad1a06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "46febe8f-a90b-49dc-9791-c73ce2d980f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(solver='saga', max_iter=500, C=.1,\n",
    "                        class_weight=\"balanced\", penalty=\"l1\", # \"elasticnet\"\n",
    "                       )\n",
    "classifier = MultiOutputClassifier(lr, n_jobs=-1)\n",
    "classifiers[\"lr_l1_C1\"] = classifier"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8ce69cac-5288-4c31-a195-ccd52837e1c6",
   "metadata": {},
   "source": [
    "#@l2 not work even with C=0.1, maybe need go very small?\n",
    "#@but anyway maybe l1 is better choice\n",
    "lr = LogisticRegression(solver='sag', max_iter=500, C=0.1, \n",
    "                        class_weight=\"balanced\", penalty=\"l2\"\n",
    "                       )\n",
    "classifier = MultiOutputClassifier(lr, n_jobs=-1)\n",
    "classifiers[\"lr_l2_C.1\"] = classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "8886294c-4fe8-47a4-b605-ec68301ed931",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC(class_weight='balanced', # probability=True, \n",
    "          C=1, kernel='rbf' # 'sigmoid', 'linear', 'poly' \n",
    "         )\n",
    "classifier = MultiOutputClassifier(svc, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b62a63-8c05-4eaf-925a-6671969f9134",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "a73f6137-a5d6-4f14-afbf-3281468d098a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 211/211 [00:14<00:00, 14.63it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.542349356804333"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 211/211 [00:16<00:00, 13.17it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7126269465132025"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 211/211 [00:14<00:00, 14.70it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7381855111712932"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 211/211 [00:14<00:00, 14.37it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.626325885804559"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 211/211 [00:19<00:00, 11.10it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6557887610020311"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_metric(classifier, X_test, y_test, top_n=10)\n",
    "custom_metric(classifier, X_test, y_test, top_n=20)\n",
    "custom_metric(classifier, X_test, y_test, top_n=25)\n",
    "\n",
    "custom_metric(classifier, X_test, y_test, top_n=.5)\n",
    "custom_metric(classifier, X_test, y_test, top_n=.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee0786e-ec5f-4870-b205-feb081bcc1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_n = np.random.choice(X_test.shape[0])\n",
    "X_test_rand = X_test[rand_n,:]\n",
    "y_test_rand = y_test.iloc[rand_n,:] \n",
    "print(\"Real JELs: \", y_test_rand[y_test_rand > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf4240e-1b87-4b91-8cf3-d72bf8d1f287",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_proba_rand = classifier.predict_proba(X_test_rand) \n",
    "predict_proba_rand = np.array([a[0][1] for a in predict_proba_rand])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85eb6f8-db99-40e3-b54f-3184965d127f",
   "metadata": {},
   "outputs": [],
   "source": [
    "jel_rank = np.argsort(predict_proba_rand)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "id": "4a36f642-1c2f-4c2a-9a04-3a00b96e2f8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['O25', 'D25', 'E21', 'B51', 'E22', 'F40', 'Q22', 'E25', 'F43',\n",
       "       'E01', 'B41', 'D14', 'H30', 'Q51', 'E24'], dtype=object)"
      ]
     },
     "execution_count": 374,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 0.99999991,\n",
       "       0.99999666, 0.99999572, 0.99614458, 0.96969067, 0.81578492])"
      ]
     },
     "execution_count": 374,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_n = (predict_proba_rand > 0.5).sum()\n",
    "jel_labels[jel_rank][:top_n]\n",
    "predict_proba_rand[jel_rank][:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "e2e53c3d-b5d3-405d-80f1-4d4ab28e385a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['O25', 'D25', 'E21', 'B51', 'E22', 'F40', 'Q22', 'E25', 'F43',\n",
       "       'E01', 'B41', 'D14', 'H30', 'Q51', 'E24', 'O18', 'O41', 'J24',\n",
       "       'D24', 'H23'], dtype=object)"
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 0.99999991,\n",
       "       0.99999666, 0.99999572, 0.99614458, 0.96969067, 0.81578492,\n",
       "       0.38942712, 0.27405326, 0.17228893, 0.04896066, 0.00566725])"
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_n = 20\n",
    "jel_labels[jel_rank][:top_n]\n",
    "predict_proba_rand[jel_rank][:top_n]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da6c8a0-5c03-49de-92dc-77e26a5f4aa4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### ???\n",
    "\n",
    "https://keras.io/examples/nlp/multi_label_classification/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb073e0d-df10-48b9-be2d-46b2b96f6b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "# import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86b7f92-f33b-48a6-8cba-67795e7f8bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model():\n",
    "    shallow_mlp_model = keras.Sequential(\n",
    "        [\n",
    "            layers.Dense(512, activation=\"relu\"),\n",
    "            layers.Dense(256, activation=\"relu\"),\n",
    "            layers.Dense(lookup.vocabulary_size(), activation=\"sigmoid\"),\n",
    "        ]  # More on why \"sigmoid\" has been used here in a moment.\n",
    "    )\n",
    "    return shallow_mlp_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3933b002-5ae5-4638-9234-08b87fea8a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "\n",
    "shallow_mlp_model = make_model()\n",
    "shallow_mlp_model.compile(\n",
    "    loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"categorical_accuracy\"]\n",
    ")\n",
    "\n",
    "history = shallow_mlp_model.fit(\n",
    "    train_dataset, validation_data=validation_dataset, epochs=epochs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5809b479-5caa-4e99-940e-e0151d7501d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba04cf0a-1bd8-49b8-9ddb-8a93d503e993",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=5000, lower=True)\n",
    "tokenizer.fit_on_texts(abstract)\n",
    "sequences = tokenizer.texts_to_sequences(abstract) # transforms the words in numbers\n",
    "X = pad_sequences(sequences, maxlen=200) # ensures all the vectors have the same length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3641150-423a-4a27-a8b1-be9928a2c669",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d9a26ca3-13c5-40a0-a9ee-47ee3e9a3663",
   "metadata": {},
   "source": [
    "## Try Unsupervised Algorithms\n",
    "\n",
    "- @ not extremely bad, but far away from useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ddf8ec7d-75eb-4267-82e4-bb350fcd597a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c55b19a5-283b-42d3-b77a-53b547988f98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 42052/42052 [14:04<00:00, 49.82it/s]\n"
     ]
    }
   ],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "def tokenize_w(s):\n",
    "    words = word_tokenize(s)\n",
    "    words = [w for w in words if w in tokens]\n",
    "    return words\n",
    "    \n",
    "def tokenize_s(p):\n",
    "    sentences = sent_tokenize(p) \n",
    "    sentences = [tokenize_w(s) for s in sentences]\n",
    "    return sentences\n",
    "    \n",
    "corpus_ = Parallel(n_jobs=-1)(delayed(tokenize_s)(abs.lower()) for abs in tqdm(df.abstract))\n",
    "corpus = [s for p in corpus_ for s in p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "049ae67e-75ff-42c6-bca8-b18bb3cacaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "model = Word2Vec(\n",
    "    sentences=corpus,\n",
    "    vector_size=300, \n",
    "    window=5,\n",
    "    min_count=2,\n",
    "    workers=7,\n",
    "    sg=0,  # 1 for skip-gram; otherwise CBOW.\n",
    "    hs=0,  # 1 for hierarchical softmax used for model training; 0 for non-zero, negative sampling\n",
    "    negative=5,  # specifies how many “noise words” drawn (usually between 5-20) in negative sampling, default 5\n",
    "    ns_exponent=0.75,  # exponent used to shape the negative sampling distribution. A value of 1.0 samples exactly in proportion to the frequencies, 0.0 samples all words equally, popular default value of 0.75\n",
    "    cbow_mean=1,  # If 0, use the sum of the context word vectors. If 1, use the mean\n",
    ")\n",
    "\n",
    "wv = model.wv #.save(f\"../Output/word2vec_v{vector_size}_w{window}_occ{i}.wordvectors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "a46bef6e-31f5-48c0-927d-70eabf0f201c",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_p = [np.concatenate(p) for p in corpus_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "cf8f1cfe-3235-4572-a5f5-e9a274048a81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 42052/42052 [01:18<00:00, 534.52it/s]\n"
     ]
    }
   ],
   "source": [
    "corpus_p_ = []\n",
    "for p in tqdm(corpus_p):\n",
    "    p = [w for w in p if w in wv.index_to_key]\n",
    "    corpus_p_.append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "8b1b22ef-773e-4fa5-8a23-24b2b9c59bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_p = [np.mean(wv[p], axis=0) for p in corpus_p_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacede1d-cb20-43db-8cd6-2702a87e0cf7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "02772162-e998-46a1-a896-f9fe66d12ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from xml.etree import ElementTree\n",
    "\n",
    "response = requests.get(\"https://www.aeaweb.org/econlit/classificationTree.xml\")\n",
    "tree = ElementTree.fromstring(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "b3eb2742-9148-4e71-813b-fefdc71f1908",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def etree_to_dict(t):\n",
    "    d = {t.tag: {} if t.attrib else None}\n",
    "    children = list(t)\n",
    "    if children:\n",
    "        dd = defaultdict(list)\n",
    "        for dc in map(etree_to_dict, children):\n",
    "            for k, v in dc.items():\n",
    "                dd[k].append(v)\n",
    "        d = {t.tag: {k: v[0] if len(v) == 1 else v\n",
    "                     for k, v in dd.items()}}\n",
    "    if t.attrib:\n",
    "        d[t.tag].update(('@' + k, v)\n",
    "                        for k, v in t.attrib.items())\n",
    "    if t.text:\n",
    "        text = t.text.strip()\n",
    "        if children or t.attrib:\n",
    "            if text:\n",
    "                d[t.tag]['#text'] = text\n",
    "        else:\n",
    "            d[t.tag] = text\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "803a5f01-e979-488b-a0c6-476ca7fe488e",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = etree_to_dict(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "d3f93476-d137-4947-9a05-2c035acb2298",
   "metadata": {},
   "outputs": [],
   "source": [
    "d3 = [[d2['classification'],d2[\"description\"]] for d1 in d['data']['classification'] for d2 in d1['classification']] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "af25a401-fdb8-4f42-a4ca-7af1c3ffad27",
   "metadata": {},
   "outputs": [],
   "source": [
    "d3 = [[d_[\"code\"],d2des+\" @ \"+d_[\"description\"]] for d__, d2des in d3 for d_ in d__ if isinstance(d_, dict)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "3fca0a62-7546-4088-af0c-4cfa652f98fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "jel_official = pd.DataFrame(d3, columns=[\"jel\",\"des\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "30e551e8-c31f-43ce-bed4-ac8500643b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "des = jel_official.des.apply(stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "15bfba93-3003-4612-bd43-e84e32f17bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_o = [tokenize_w(s.lower()) for s in des]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "fff6c87e-b50c-40bb-aa4f-a33e64d8e4a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 839/839 [00:01<00:00, 545.33it/s]\n"
     ]
    }
   ],
   "source": [
    "corpus_o_ = []\n",
    "for p in tqdm(corpus_o):\n",
    "    p = [w for w in p if w in wv.index_to_key]\n",
    "    corpus_o_.append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "890b65b9-72f8-43b7-a1bc-f409c7b521cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "jel_official[\"corpus\"] = corpus_o_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "b08892c4-b970-45c1-b3b3-c48b54a1356f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = jel_official.corpus.apply(lambda x: len(x)==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "5197fd6c-a177-48a3-a74e-9a35de5ff410",
   "metadata": {},
   "outputs": [],
   "source": [
    "jel_official = jel_official[~mask].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "3a71d5e6-8347-4979-bd77-5e1485f8a791",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_o = [np.mean(wv[p], axis=0) for p in jel_official.corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "d6ce2093-86cd-4228-9442-48fa711211e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import spatial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "06c7f52e-362e-4e03-8874-35ef7c3f3ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_n = np.random.choice(df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "a38f8716-edaf-484b-baae-5fb51b96d716",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['J11', 'C13', 'C14', 'C15', 'C53', 'M52', 'C12', 'C19', 'C02',\n",
       "       'C43', 'C40', 'C11', 'C49', 'C18', 'C45', 'C46', 'C41', 'C10',\n",
       "       'C26', 'C36'], dtype=object)"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarities = [1 - spatial.distance.cosine(latent_p[rand_n], lo) for lo in latent_o]\n",
    "jel_rank = np.argsort(similarities)[::-1]\n",
    "jel_official.jel[jel_rank].values[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "e9e426d0-41c9-497a-b59b-216ccb75ad6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "I20    1\n",
       "I21    1\n",
       "J24    1\n",
       "Name: 153726, dtype: int64"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_jel = jel_dummy_matrix.iloc[rand_n,:] \n",
    "y_jel[y_jel > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7317782d-26bf-4756-bd01-980ad2d56b04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
